{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deeper Analysis and Further Experiments\n",
    "Analysis of TF-IDF vectorization on the dataset and experiments with different vector encoding methods outside the scope of the challenge, which I did on my own time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While TF-IDF was sufficient for the purpose of this challenge, there were several key things I noticed. To start, using TF-IDF and cosine similarity was excellent in comparing sentence similarity in a lexical sense such that if two sentences or phrases share common vocabulary, the algorithm would usually pick it up. However, this is also where the shortcoming come in. If the ordering of the words were changed or some synonyms are used, the program would sometimes struggle to return the correct results.\n",
    "\n",
    "Take these two phrases as example using the `main.py` script.\n",
    "\n",
    "Query: `psycho horror movies`\n",
    "```txt\n",
    "Results\n",
    "Movie: Psycho --- Similarity: 0.2611649965940957\n",
    "Movie: A Night at the Opera --- Similarity: 0.08203814048662973\n",
    "Movie: What Ever Happened to Baby Jane? --- Similarity: 0.06942657526655543\n",
    "Movie: All Quiet on the Western Front --- Similarity: 0.06685645193757431  \n",
    "Movie: The Exorcist --- Similarity: 0.06444365239730257\n",
    "```\n",
    "\n",
    "Query: `horror psycho movies`\n",
    "```txt\n",
    "Results\n",
    "Movie: Slumdog Millionaire --- Similarity: 0.0\n",
    "Movie: The Straight Story --- Similarity: 0.0 \n",
    "Movie: His Girl Friday --- Similarity: 0.0    \n",
    "Movie: Short Term 12 --- Similarity: 0.0      \n",
    "Movie: The Lost Weekend --- Similarity: 0.0 \n",
    "```\n",
    "\n",
    "As one can see, while the queries were similar lexically, they gave completely different results. In fact, the second query resulted in no matches found, even though it had essentially the same meaning as the first one. What this shows is that for tasks such as content recommendation, TF-IDF and lexical analysis can only go so far. What we need are vector embeddings that can capture the semantic meaning of the data and the queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SBERT\n",
    "After doing further research, I found a pre-trained embedding model that can capture a sentence's semantic meaning into vector form. This model, SBERT, is built on top of the BERT model, which utilizes transformers to capture information within words and sentences. Since SBERT is a complex neural network, this was not included in the main submission; however, it was still interesting to see the improvements involving this change.\n",
    "\n",
    "Using the Juypter code below and the same queries from above:\n",
    "\n",
    "Query: `psycho horror movies`\n",
    "```txt\n",
    "Movie: Psycho --- Similarity: 0.6416359543800354\n",
    "Movie: Touch of Evil --- Similarity: 0.5505009889602661\n",
    "Movie: The Exorcist --- Similarity: 0.5373932123184204\n",
    "Movie: Rope --- Similarity: 0.5182092189788818\n",
    "Movie: The Shining --- Similarity: 0.5134167075157166\n",
    "```\n",
    "\n",
    "Query: `horror psycho movies`\n",
    "```txt\n",
    "Movie: Psycho --- Similarity: 0.6472702622413635\n",
    "Movie: Touch of Evil --- Similarity: 0.551189661026001\n",
    "Movie: The Exorcist --- Similarity: 0.529065728187561\n",
    "Movie: The Shining --- Similarity: 0.5264713168144226\n",
    "Movie: Rope --- Similarity: 0.5127915740013123\n",
    "```\n",
    "\n",
    "Because these two queries were semantically the same, SBERT was able to capture this and returned similar outputs after calculating cosine similarity. This is a stark difference from what was outputed in the TF-IDF pipeline.\n",
    "\n",
    "Below is the Juypter code for my SBERT prompting pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the sentence-transformer package\n",
    "%pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use one of SBERT's pretrained sentence transformer\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training set and add All_Info column, same as in main.py\n",
    "df = pd.read_csv('./data/imdb_top250_movies.csv')\n",
    "all_info = df[[\"Title\", \"Genre\", \"Director\", \"Actors\", \"Plot\", \"Language\"]].values.tolist()\n",
    "    \n",
    "for i, info in enumerate(all_info):\n",
    "    all_info[i] = \" \".join(info)\n",
    "\n",
    "df[\"All_Info\"] = all_info\n",
    "\n",
    "sentences = df[\"All_Info\"].to_numpy()\n",
    "vectors = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompting block\n",
    "\n",
    "# Enter prompt here _=============================\n",
    "prompt = \"return a list of superhero movies\"\n",
    "# ================================================\n",
    "\n",
    "prompt_vector = model.encode(prompt)\n",
    "similarity = cosine_similarity([prompt_vector], vectors).flatten()\n",
    "\n",
    "idx = np.argsort(similarity)[-5:][::-1]\n",
    "titles = df[\"Title\"].to_numpy()[idx]\n",
    "\n",
    "for i in range(len(idx)):\n",
    "    print(f\"Movie: {titles[i]} --- Similarity: {similarity[idx][i]}\")\n",
    "print(\"===========================================\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
